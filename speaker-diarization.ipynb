{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8814483,"sourceType":"datasetVersion","datasetId":5238724},{"sourceId":8929832,"sourceType":"datasetVersion","datasetId":5371813},{"sourceId":9025322,"sourceType":"datasetVersion","datasetId":5439318},{"sourceId":9025569,"sourceType":"datasetVersion","datasetId":5439488}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Automatic Speech Recognition with Speaker Diarization","metadata":{}},{"cell_type":"code","source":"!pip install wget\n!apt-get -y install sox libsndfile1 ffmpeg\n!pip install text-unidecode\n!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n!pip install pydub","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BRANCH = 'main'\n!python -m pip install git+https://github.com/motawie0/NeMo.git@$BRANCH#egg=nemo_toolkit[asr] ","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom IPython.display import Audio, display\nimport librosa\nimport os\nimport wget\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nimport pprint\nfrom omegaconf import OmegaConf\nimport shutil\nimport os\nimport csv\nimport argparse\nimport nemo.collections.asr as nemo_asr\nimport json\nimport nemo\nfrom nemo.collections.asr.parts.utils.decoder_timestamps_utils import ASRDecoderTimeStamps\nfrom nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR\nimport os\nfrom IPython.display import clear_output\nimport torch\npp = pprint.PrettyPrinter(indent=4)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# diarizaer","metadata":{}},{"cell_type":"code","source":"data_dir ='/kaggle/working'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DOMAIN_TYPE = \"telephonic\" # Can be meeting or telephonic based on domain type of the audio file\nCONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n\nCONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n\nif not os.path.exists(os.path.join(data_dir,CONFIG_FILE_NAME)):\n    CONFIG = wget.download(CONFIG_URL, data_dir)\nelse:\n    CONFIG = os.path.join(data_dir,CONFIG_FILE_NAME)\n\ncfg = OmegaConf.load(CONFIG)\nprint(OmegaConf.to_yaml(cfg))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = \"/kaggle/input/diarization-manifest/input_manifest.json\"\noutput_path = \"/kaggle/working/split_manifest\"\nlines_per_part = 1  # Set this to the number of lines you want per file\n\n# Ensure the output directory exists\nos.makedirs(output_path, exist_ok=True)\n\ndef split_json_file(input_path, output_path, lines_per_part):\n    # Read all lines from the file\n    with open(input_path, 'r') as file:\n        lines = file.readlines()\n\n    # Calculate the number of parts needed\n    total_lines = len(lines)\n    num_parts = (total_lines + lines_per_part - 1) // lines_per_part  # Ensure all lines are covered\n    print(f\"Total lines: {total_lines}, Lines per part: {lines_per_part}, Total parts: {num_parts}\")\n\n    # Split and write to new files\n    for part in range(num_parts):\n        start = part * lines_per_part\n        end = min(start + lines_per_part, total_lines)  # Avoid going out of range\n        part_file_path = f\"{output_path}/part_{part + 1}.json\"\n\n        # Write the current part to its file\n        with open(part_file_path, 'w') as part_file:\n            for line in lines[start:end]:\n                part_file.write(line)\n        print(f\"Part {part + 1} written to {part_file_path}\")\n\n# Example usage\nsplit_json_file(input_path, output_path, lines_per_part)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_speaker_model='titanet_large'\ncfg.diarizer.out_dir = data_dir #Directory to store intermediate files and prediction outputs\ncfg.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\ncfg.diarizer.clustering.parameters.oracle_num_speakers=False\ncfg.batch_size=1\ncfg.diarizer.msdd_model.parameters.infer_batch_size=1\ncfg.diarizer.asr.parameters.asr_batch_size=1\n# Using Neural VAD and Conformer ASR \ncfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'\ncfg.diarizer.asr.model_path = \"/kaggle/input/the-best-results/results/Some name of our experiment/checkpoints/conformer.nemo\" \ncfg.diarizer.oracle_vad = False # ----> Not using oracle VAD \ncfg.diarizer.asr.parameters.asr_based_vad = False\n# cfg.diarizer.asr.ctc_decoder_parameters.pretrained_language_model = '/kaggle/working/5gram.bin'\ncfg.diarizer.ignore_overlap=False","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming that you are using CUDA\nimport json\ntoo_big = []\nfor manifest_file in os.listdir(\"/kaggle/working/split_manifest\"):\n    file_path = f'/kaggle/working/split_manifest/{manifest_file}'\n    print(file_path)\n    with open(file_path, 'r') as file:\n        dur = json.load(file)['duration']\n    if dur > 200:\n        too_big.append(file_path)\n        continue\n    cfg.diarizer.manifest_filepath = file_path\n    asr_decoder_ts = ASRDecoderTimeStamps(cfg.diarizer)\n    asr_model = asr_decoder_ts.set_asr_model()\n    word_hyp, word_ts_hyp = asr_decoder_ts.run_ASR(asr_model)\n    asr_diar_offline = OfflineDiarWithASR(cfg.diarizer)\n    asr_diar_offline.word_ts_anchor_offset = asr_decoder_ts.word_ts_anchor_offset\n    diar_hyp, diar_score = asr_diar_offline.run_diarization(cfg, word_ts_hyp)\n    trans_info_dict = asr_diar_offline.get_transcript_with_speaker_labels(diar_hyp, word_hyp, word_ts_hyp)\n    clear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transcribe_audio(checkpoint_path, data_dir, output_csv='transcriptions.csv', batch_size=4):\n    # Restore the ASR model from the checkpoint\n    asr_model = nemo_asr.models.EncDecCTCModel.restore_from(checkpoint_path)\n\n    # List all .wav files in the directory\n    wav_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n\n    # Prepare the list of audio paths\n    audio_paths = [os.path.join(data_dir, wav) for wav in wav_files]\n\n    # Transcribe the audio files in batches\n    transcriptions = []\n    for i in range(0, len(audio_paths), batch_size):\n        batch_paths = audio_paths[i:i + batch_size]\n        transcripts = asr_model.transcribe(audio=batch_paths, batch_size=len(batch_paths))\n        transcriptions.extend(transcripts)\n    print(transcriptions)\n    # Prepare data for CSV\n    csv_data = []\n    for wav, transcript in zip(wav_files, transcriptions):\n        audio_name = os.path.splitext(wav)[0]\n        csv_data.append([audio_name, transcript])\n\n    # Write to CSV\n    with open(output_csv, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['audio', 'transcript'])\n        writer.writerows(csv_data)\n\n    print(f\"Transcriptions saved to {output_csv}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from omegaconf import OmegaConf, DictConfig\nfrom omegaconf import OmegaConf\nimport wget\n# msdd_model_path ='/kaggle/input/msdd-10-epochs-on-3k3h-2-4/nemo_experiments/MultiscaleDiarDecoder/2024-07-13_12-33-15/checkpoints/MultiscaleDiarDecoder--val_loss=0.8137-epoch=1.ckpt' \n! git clone https://github.com/NVIDIA/NeMo.git\n!rm /kaggle/working/diar_infer_telephonic.yaml\nMODEL_CONFIG = os.path.join(\"/kaggle/working/\",'diar_infer_telephonic.yaml')\nif not os.path.exists(MODEL_CONFIG):\n    config_url = \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_telephonic.yaml\"\n    MODEL_CONFIG = wget.download(config_url,\"/kaggle/working/\")\n\noutput_dir = \"/kaggle/working/output_inference\"\nconfig = OmegaConf.load(MODEL_CONFIG)\nconfig.diarizer.out_dir=\"/kaggle/working/output_inference\"\nconfig.diarizer.manifest_filepath='/kaggle/working/input_manifest.json'\nconfig.diarizer.oracle_vad=False\nconfig.diarizer.clustering.parameters.oracle_num_speakers = False\nOmegaConf.save(config, \"/kaggle/working/diar_infer_telephonic.yaml\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef crop_audio(input_wav, start_ms, end_ms):\n    audio = AudioSegment.from_wav(input_wav)\n    audio = audio.set_frame_rate(16000)\n    cropped_audio = audio[start_ms*1000:end_ms*1000]\n    cropped_audio.export(\"/kaggle/working/temp_wav_output/croped_file.wav\", format=\"wav\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/temp_wav_output\n!mkdir /kaggle/working/temp_wav\n!mkdir /kaggle/working/long_audio_json\nasr_model = nemo_asr.models.EncDecCTCModel.restore_from(\"/kaggle/input/the-best-results/results/Some name of our experiment/checkpoints/conformer.nemo\")\n\nfor file in too_big:\n    !rm -rf /kaggle/working/output_inference/pred_rttms\n    with open(file, 'r') as file:\n        audio_path = json.load(file)['audio_filepath']\n    meta = {\n    'audio_filepath': audio_path, \n    'offset': 0, \n    'duration':None, \n    'label': 'infer', \n    'text': '-', \n    'num_speakers': None, \n    'rttm_filepath': None, \n    'uem_filepath' : None\n    }\n    with open('input_manifest.json','w') as fp:\n        json.dump(meta,fp)\n        fp.write('\\n')\n    \n    !HYDRA_FULL_ERROR=1 python /kaggle/working/NeMo/examples/speaker_tasks/diarization/neural_diarizer/multiscale_diar_decoder_infer.py --config-path /kaggle/working --config-name diar_infer_telephonic.yaml\n    rttm_file_path = os.listdir(\"/kaggle/working/output_inference/pred_rttms\")\n    data = []\n    with open(\"/kaggle/working/output_inference/pred_rttms/\"+rttm_file_path[0], 'r') as file_pred_rttm:\n        for line in file_pred_rttm:\n            parts = line.strip().split()\n            if parts[0] == \"SPEAKER\":\n                # Extract the start time and duration\n                start_time = float(parts[3])\n                end = start_time +  float(parts[4])\n                speaker = parts[7]\n                crop_audio(audio_path,start_time,end)\n                transcripts = asr_model.transcribe(audio=\"/kaggle/working/temp_wav_output/croped_file.wav\", batch_size=1)[0]\n                data.append((start_time,end,speaker,transcripts))\n    segments = [{\"start\": start, \"end\": end, \"speaker\": speaker, \"text\": text} for start, end, speaker, text in data]\n    audio_filename = os.path.splitext(os.path.basename(audio_path))[0]\n    output_json_path = f'/kaggle/working/long_audio_json/{audio_filename}.json'\n    print(f\"dum in {output_json_path}\")\n    # Write the JSON data to a file\n    with open(output_json_path, 'w', encoding='utf-8') as f:\n        json.dump(segments, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport re\n\n# Function to convert time in \"MM:SS.SS\" format to seconds\ndef time_to_seconds(minutes, seconds):\n    return int(minutes) * 60 + float(seconds)\n\n# Function to process a single file and convert it to JSON\ndef process_file(input_file_path, output_file_path):\n    with open(input_file_path, 'r', encoding='utf-8') as file:\n        data = file.read()\n    \n    # Regular expression to parse the data\n    pattern = re.compile(r'\\[(\\d{2}):(\\d{2}\\.\\d{2}) - (\\d{2}):(\\d{2}\\.\\d{2})\\] (speaker_\\d+): (.+)')\n\n    # Parse the input data and convert to JSON format\n    segments = []\n    for match in pattern.finditer(data):\n        start_minutes, start_seconds, end_minutes, end_seconds, speaker, text = match.groups()\n        start_time = time_to_seconds(start_minutes, start_seconds)\n        end_time = time_to_seconds(end_minutes, end_seconds)\n        segments.append({\n            \"start\": start_time,\n            \"end\": end_time,\n            \"speaker\": speaker,\n            \"text\": text\n        })\n\n    # Write the JSON data to a file\n    with open(output_file_path, 'w', encoding='utf-8') as f:\n        json.dump(segments, f, ensure_ascii=False, indent=4)\n\n# Paths to the input and output directories\ninput_directory = '/kaggle/working/pred_rttms'\noutput_directory = '/kaggle/working/convert_to_jason_small_audio'\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_directory, exist_ok=True)\n\n# Process each .txt file in the input directory\nfor filename in os.listdir(input_directory):\n    if filename.endswith('.txt'):\n        input_file_path = os.path.join(input_directory, filename)\n        output_file_name = os.path.splitext(filename)[0] + '.json'\n        output_file_path = os.path.join(output_directory, output_file_name)\n        \n        # Process the file and convert to JSON\n        process_file(input_file_path, output_file_path)\n\n        print(f\"Processed {input_file_path} -> {output_file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/convert_to_jason_small_audio\n!ls /kaggle/working/long_audio_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}